#!/usr/bin/env python3
#
# complementary_environmental_variables.py
#
# This program downloads data from OpenStreetMap to put together environmental
# variables that are complementary to the locations of the images in the
# supplied GeoJSON input file such as the survey data generated by the Percept
# project:
#   https://github.com/Spatial-Data-Science-and-GEO-AI-Lab/percept
# More details can be found in the README.md file.
#
# Copyright (2024): Matthew Danish
# This program is free software: you can redistribute it and/or modify it under
# the terms of the GNU General Public License as published by the Free Software
# Foundation, either version 3 of the License, or (at your option) any later
# version.
# 
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
# FOR A PARTICULAR PURPOSE. See the GNU General Public License for more
# details.
import os
import sys
import time
import bz2
#from tqdm import tqdm
from tqdm_loggable.auto import tqdm
import math
import pyproj
import requests
import json
import networkx as nx
import numpy as np
import osmnx as ox
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
import contextily as ctx
import concurrent
from pathlib import Path
import argparse
import psutil

parser = argparse.ArgumentParser(
                    prog='complementary_environmental_variables',
                    description='analyse complementary environmental variables')

parser.add_argument('--map', action='store_true', help='output a map', default=False)
parser.add_argument('--write-only', action='store_true', help='only save to  files, do not read from them', default=False)
parser.add_argument('--all-maps', action='store_true', help='output all maps', default=False)
parser.add_argument('--subgraphs', metavar='NETWORK_TYPE', type=str, default=None, help='do subgraph partitioning on all|walk|bike networks')
parser.add_argument('--specific-subgraph', metavar='INDEX', type=int, default=None, help='look at a particular subgraph index only')
parser.add_argument('--features', action='store_true', help='do features output', default=False)
parser.add_argument('--subgraph-stats', action='store_true', help='do subgraph stats output', default=False)
parser.add_argument('--stats-gdf-output', action='store_true', help='do subgraph stats output to summary GDF', default=False)
parser.add_argument('--quiet', action='store_true', help='quiet mode', default=False)
parser.add_argument('--buffer-size', metavar='METERS', type=int, default=300, help='Buffer size (radius in meters) for analysis around each geographic point')
parser.add_argument('--crs', metavar='EPSG_NUM', type=int, default=28992, help='EPSG coordinate reference system number (see https://epsg.io/ (default 28992)')
parser.add_argument('--geojson', metavar='URL', default=None, help='URL to download GeoJSON file with geographic points to analyze.')
parser.add_argument('--geojson-url-in-file', metavar='FILE', default='url.txt', help='Get GeoJSON URL from this file if --geojson not specified.')
parser.add_argument('--default-speed-limit', metavar='KM/H', default=50, help='Default speed limit in area (km/h)')
parser.add_argument('--bbox', metavar='N,S,E,W', default='52.4284,52.2818,5.1220,4.7149', help='Bounding box of interest (north, south, east, west) for OSMnx v1 [in OSMnx v2.0.0+ this becomes (left, bottom, right, top)]')
parser.add_argument('--max-workers', metavar='COUNT', type=int, default=None, help='Number of processes to create when doing analysis. Default: os.cpu_count()')
parser.add_argument('--basedir', metavar='DIR', default=None, help='Directory to store cached and saved files in')
args = parser.parse_args()

def log(s, level=1, flush=False):
    if args.quiet and level > 0: return
    print(s, flush=flush)

# GeoJSON data URL:
if args.geojson is not None:
    url = args.geojson
elif args.geojson_url_in_file is not None:
    with open(args.geojson_url_in_file) as fp:
        url = fp.read().strip()
else:
    log('No GeoJSON url supplied', level=0)
    sys.exit(1)

# EPSG coordinate reference system number, see: https://epsg.io/
# e.g. https://epsg.io/28992 for the Netherlands
epsg = args.crs
buffer_size = args.buffer_size
default_speed_limit = args.default_speed_limit
bbox = list(map(float,args.bbox.split(',')))


log(f"""GeoJSON URL: {url}
Buffer Size: {buffer_size}
Coordinate Reference System: {epsg}
Bounding Box: {bbox}
Default Speed Limit: {default_speed_limit}""")

if args.basedir is None:
    basedir = Path(os.getcwd()) / "cev"
else:
    basedir = args.basedir
basedir.mkdir(parents=True, exist_ok=True)

if args.subgraphs not in [None, 'walk', 'all', 'bike']:
    sys.exit(1)

network_type = args.subgraphs

if network_type is not None:
    network_path = basedir / f"network_{network_type}.gpkg"
    network_graphml_path = basedir / f"network_{network_type}.graphml"

    log(f"Base directory: {basedir}\nnetwork_{network_type} GeoPackage: {network_path}\nnetwork_{network_type} GraphML: {network_graphml_path}")
    if network_graphml_path.exists():
        log("GraphML file already exists. Skipping download of street network.")
        G = ox.load_graphml(network_graphml_path)
    else:
        # Download street network
        G = ox.graph_from_bbox(bbox=bbox, network_type=network_type, truncate_by_edge=True, retain_all=True)
        # Save to graphml
        ox.io.save_graphml(G, network_graphml_path)

    if not network_path.exists():
        # Save to GeoPackage
        ox.save_graph_geopackage(G, network_path)

    nodes_proj, edges = ox.graph_to_gdfs(G, nodes=True, edges=True)
    bbox_env = edges.unary_union.envelope
    bbox_env_buffer = bbox_env.buffer(0.08, cap_style=3, join_style=2)
    envgdf = gpd.GeoDataFrame(geometry=gpd.GeoSeries(bbox_env_buffer))


"""## Utility functions"""



dataset_path = basedir / "dataset.gpkg"

def load_points(url):
    if dataset_path.exists():
        log(f'load_points: loading existing {dataset_path}')
        return gpd.read_file(dataset_path)
    else:
        # Download the GeoJSON data
        response = requests.get(url)
        data = response.json()

        # Create a GeoPandas DataFrame from the geojson data.
        gdf = gpd.GeoDataFrame.from_features(data["features"])
        gdf.crs = pyproj.CRS(4326)

        # convert the CRS to a projected CRS
        gdf = gdf.to_crs(pyproj.CRS(epsg))
        gdf.to_file(dataset_path, driver="GPKG")
        return gdf

buffers_path = basedir / f"buffers_{buffer_size}m.gpkg"

def points_to_buffers(gdf):
    if buffers_path.exists():
        log(f'points_to_buffers: loading existing {buffers_path}')
        return gpd.read_file(buffers_path)
    gdf = gdf.copy()
    # Ensure correct coordinate reference system
    gdf.crs = pyproj.CRS(epsg)
    # Buffer the points (convert to polygon forming a circle at radius buffer_size around each point)
    gdf['geometry'] = gdf['geometry'].buffer(buffer_size)
    gdf.to_file(buffers_path, driver="GPKG")
    return gdf

gdf = load_points(url)
buffered_gdf = points_to_buffers(gdf)


def show_on_map(gdf_or_poly, show_osmid=False):
    if type(gdf_or_poly) == gpd.GeoDataFrame:
        gdf = gdf_or_poly
    else:
        gdf = gpd.GeoDataFrame(geometry=[gdf_or_poly], crs=epsg)
    if gdf.empty: return

    # Make a copy of gdf
    gdf = gdf.copy()

    # Reproject the GeoDataFrame to a web mercator coordinate system for compatibility with contextily
    gdf = gdf.to_crs(epsg=3857)

    # Create a map from the GeoDataFrame's geometry

    ax = gdf.plot(figsize=(10, 10), alpha=0.5, edgecolor='k')
    if show_osmid:
        # with labels based on gdf['osmid']
        for idx, row in gdf.iterrows():
            ax.annotate(text=str(row['osmid']), xy=row['geometry'].centroid.coords[0], ha='center')

    # Add a basemap from OpenStreetMap
    ctx.add_basemap(ax)

def clean_maxspeed(v):
    if type(v) == str:
        if len(v) == 0:
          return default_speed_limit
        elif v[0] == '[':
            return np.median([clean_maxspeed(v2) for v2 in v[1:-1].split(',')])
        else:
            return float(v.strip(' \'')) # ox.routing._clean_maxspeed(v)
    elif type(v) == list:
        if len(v) == 0:
            return default_speed_limit
        else:
            return np.median([clean_maxspeed(v2) for v2 in v])
    else:
        return (float(v))

def calc_medspeed(G):
    edges = ox.graph_to_gdfs(G, nodes=False, edges=True)
    if 'maxspeed' in edges.columns:
        return edges['maxspeed'].dropna().apply(clean_maxspeed).median()
    else:
        return clean_maxspeed([])

def orientation_entropy(G, undirected_G=None):
    if undirected_G is None:
        Gu = ox.convert.to_undirected(G)
    else:
        Gu = undirected_G
    Gub = add_edge_bearings(Gu)
    return ox.bearing.orientation_entropy(Gub)

##################################################

# A version of add_edge_bearings that does not rely on the 'node' graph at all, only on the 'edge' graph
# This way it is happy to work with the result of the geopandas.clip method on the edge graph
def add_edge_bearings(G):
    if ox.projection.is_projected(G.graph["crs"]):  # pragma: no cover
        msg = "graph must be unprojected to add edge bearings"
        raise ValueError(msg)

    uvkd = [(u, v, k, d) for u, v, k, d in G.edges(keys=True, data=True) if u != v]
    uvk = [(u, v, k) for u, v, k, d in uvkd]

    # extract endpoint coordinates from the edge geometry
    coords=np.stack(gpd.GeoDataFrame(map(lambda x: x[3], uvkd))['geometry'].explode(index_parts=False).apply(lambda x: np.array([x.coords[0], x.coords[-1]])).to_numpy())

    # calculate bearings then set as edge attributes
    bearings = ox.bearing.calculate_bearing(coords[:, 0, 1], coords[:, 0, 0], coords[:, 1, 1], coords[:, 1, 0])
    values = zip(uvk, bearings.round(1))
    nx.set_edge_attributes(G, dict(values), name="bearing")

    return G

# A version of basic_stats containing only those we are interested in, and
# avoiding problems with circuity caused by invariants broken when using the
# geopandas.clip method on the node graph.
def basic_stats(G, undirected_G=None):
    if len(G.edges) == 0: return {}
    spn = ox.stats.count_streets_per_node(G, nodes=G.nodes)
    nx.set_node_attributes(G, values=spn, name="street_count")
    if undirected_G is None:
        Gu = ox.convert.to_undirected(G)
    else:
        Gu = undirected_G
    stats = {}

    stats["num_nodes"] = int(len(G.nodes))
    stats["num_edges"] = int(len(G.edges))
    stats["streets_per_node_avg"] = float(ox.stats.streets_per_node_avg(G))
    for k, v in ox.stats.streets_per_node_proportions(G).items():
        stats[f"streets_per_node_proportions{k}"] = float(v)
    stats["intersection_count"] = int(ox.stats.intersection_count(G))
    stats["street_length_total"] = float(ox.stats.street_length_total(Gu))
    stats["street_segment_count"] = int(ox.stats.street_segment_count(Gu))
    stats["street_length_avg"] = float(stats["street_length_total"] / stats["street_segment_count"])
    return stats

##################################################

#(nodes, edges) = ox.graph_to_gdfs(G, nodes=True, edges=True)
#nodes_epsg = nodes.to_crs(epsg)
#edges_epsg = edges.to_crs(epsg)

subgraphsdir = basedir / Path(f'subgraphs_{buffer_size}m')
subgraphsdir.mkdir(parents=True, exist_ok=True)

if args.map:
    log('plotting to map.jpg')
    i = 2
    subgraphpath = subgraphsdir / Path('bike') / Path(f'{i:06d}.graphml')
    subG = ox.io.load_graphml(subgraphpath)
    show_on_map(ox.graph_to_gdfs(subG, nodes=False, edges=True))
    plt.savefig('map.jpg')
    sys.exit(0)

def mem():
    log(f'Mem used: {psutil.virtual_memory().percent}')

if args.max_workers is None:
    max_workers = os.cpu_count()
else:
    max_workers = args.max_workers

# hack because Python cannot communicate closures across interprocess communication
global_gdf = None
global_G = None
global_network_type = None

def proc_init(G, gdf, network_type):
    global global_G, global_gdf, global_network_type
    global_G = G
    global_gdf = gdf
    global_network_type = network_type

# Process a single row from the GDF table
def subgraph_from_gdf_i(i):
    # hack to workaround limitation on Python IPC / multiprocessing
    global global_G, global_gdf, global_network_type
    G = global_G
    gdf = global_gdf
    poly = gdf.iloc[i].geometry
    poly_gdf = gpd.GeoDataFrame(geometry=[poly], crs=gdf.crs)
    try:
        subgraphpath = subgraphsdir / Path(global_network_type) / Path(f'{i:06d}.graphml')
        if subgraphpath.exists() and args.specific_subgraph is None:
            subG = ox.io.load_graphml(subgraphpath)
        else:
            #subG = ox.truncate.truncate_graph_polygon(G, poly_gdf.to_crs(4326).iloc[0].geometry)
            # Use GeoPandas .clip method to extract the nodes and edges from
            # precisely within the bounds. Unfortunately, the more obvious
            # ox.truncate.truncate_graph* methods don't work very well, either
            # they drop roads that cross the boundary entirely, or they include
            # all of the road including the sections outside of the bounds.
            nodes, edges = ox.graph_to_gdfs(G, nodes=True, edges=True)
            nodes = nodes.clip(poly_gdf.to_crs(4326).iloc[0].geometry)
            edges = edges.clip(poly_gdf.to_crs(4326).iloc[0].geometry).explode(index_parts=False)
            subG = ox.graph_from_gdfs(nodes, edges)
            ox.io.save_graphml(subG, subgraphpath)
        num_edges = len(subG.edges)
        subgraphmappath = subgraphsdir / Path(global_network_type) / Path(f'{i:06d}.jpg')
        if args.all_maps and not subgraphmappath.exists():
            if num_edges > 0:
                show_on_map(ox.graph_to_gdfs(subG, nodes=False, edges=True))
            else:
                show_on_map(poly)
            plt.savefig(subgraphmappath)
        subgraphstatspath = subgraphsdir / Path(global_network_type) / Path(f'{i:06d}_stats.json')
        if args.subgraph_stats and not subgraphstatspath.exists():
            if num_edges > 0:
                subGu = ox.convert.to_undirected(subG)
                bs = basic_stats(subG, undirected_G=subGu)
                bs['orientation_entropy'] = float(orientation_entropy(subG, undirected_G=subGu))
                bs['median_speed'] = float(calc_medspeed(subG))
            else:
                bs = {'num_nodes': 0, 'num_edges': 0}
            with open(subgraphstatspath, 'w') as fp:
                json.dump(bs, fp, indent=2)

        return subG
    except Exception as e:
        msg = f'subgraph_from_gdf_i({i}): ***{type(e)}: {str(e)}'
        log(msg, level=0)
        import traceback
        log(traceback.format_exc(), level=0, flush=True)
        return msg

# Kick-off parallel map over the rows of the GDF, unless max_workers=1, then use a single-process simple for-loop.
def subgraphs_from_gdf(G, gdf, network_type):
    global global_G, global_gdf, global_network_type
    global_G = G
    global_gdf = gdf
    global_network_type = network_type
    gdf_len = gdf.shape[0]
    log(f'subgraphs_from_gdf(network_type={network_type}): extracting {gdf_len} subgraphs')
    if max_workers == 1:
        log(f'subgraphs_from_gdf(network_type={network_type}): single worker process')
        proc_init(G, gdf, network_type)
        for i in range(gdf_len):
            subgraph_from_gdf_i(i)
    else:
        log(f'subgraphs_from_gdf(network_type={network_type}): starting {max_workers} processes')
        with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers, initializer=proc_init, initargs=(G, gdf, network_type)) as executor:
            return list(executor.map(subgraph_from_gdf_i, range(gdf_len), chunksize=gdf.shape[0] // max_workers))

if args.specific_subgraph is not None:
    max_workers = 1
    proc_init(G, buffered_gdf, network_type)
    import pdb;pdb.set_trace()
    subgraph_from_gdf_i(int(args.specific_subgraph))

elif args.subgraphs:
    mem()
    (subgraphsdir / Path(network_type)).mkdir(parents=True, exist_ok=True)
    t0 = time.time()
    subGs = subgraphs_from_gdf(G, buffered_gdf, network_type)
    t1 = time.time()
    log(f'subgraphs_from_gdf: {t1 - t0:.2f}s elapsed')
    mem()

##################################################
featuresdb = {}
def get_features(name, tags):
    features_path = basedir / f"features_{name}.geojson"
    if features_path.exists():
        log(f"{name} already downloaded: skipping.")
        features = gpd.read_file(features_path)
        featuresdb[name] = features
        return features
    log(f"{name}: downloading features.")
    features = ox.features_from_bbox(bbox=bbox, tags=tags)
    #features = features.loc['node']
    features.to_file(features_path, driver="GeoJSON")
    featuresdb[name] = features
    return features

if args.features:
    get_features('greenspace', {'leisure': ['garden','fitness_station', 'common', 'dog_park', 'greenfield', 'grassland', 'heath', 'nature_reserve','park'], 'landuse': ['allotments', 'recreation_ground', 'grass', 'forest', 'greenfield','village_green', 'meadow', 'orchard'], 'natural': ['grassland','wood', 'scrub', 'heath', 'moor']}) 
    get_features('shops', {'shop': True})
    get_features('public_transport', {'public_transport': True})
    get_features('sustenance', {'amenity': ['bar', 'biergarten', 'pub', 'cafe', 'fast_food', 'food_court', 'ice_cream', 'restaurant']})
    get_features('education', {'amenity': ['college', 'dancing_school', 'driving_school', 'first_aid_school', 'kindergarten', 'language_school', 'library', 'surf_school', 'toy_library', 'research_institute', 'training', 'music_school', 'school', 'traffic_park', 'university']})
    get_features('financial', {'amenity': ['bank', 'atm', 'payment_terminal', 'bureau_de_change', 'money_transfer', 'payment_centre']})
    get_features('healthcare', {'amenity': ['baby_hatch', 'clinic', 'dentist', 'doctors', 'hospital', 'nursing_home', 'darken', 'pharmacy', 'social_facility', 'veterinary']})
    get_features('entertainment', {'amenity': ['arts_centre', 'casino', 'cinema', 'community_centre', 'conference_centre', 'events_venue', 'exhibition_centre', 'fountain', 'gambling', 'music_venue', 'nightclub', 'planetarium', 'public_bookcase', 'social_centre', 'stage', 'studio', 'theatre'], 'leisure': ['amusement_arcade', 'adult_gaming_centre', 'bowling_alley', 'bandstand', 'bathing_place', 'beach_resort', 'dance', 'disc_golf_course', 'escape_game', 'fishing', 'golf_course', 'horse_riding', 'ice_rink', 'minature_golf', 'picnic_table', 'playground', 'resort', 'sauna', 'sports_centre', 'sports_hall', 'stadium', 'summer_camp', 'swimming_area', 'swimming_pool', 'tanning_salon', 'trampoline_park', 'water_park']})


relevant_cols = ['landuse','leisure','natural','shop','public_transport','amenity']

    #get_features('signals', {'highway': 'traffic_signals'})
    #get_features('daily_shops', {'shop': ['department_store', 'supermarket', 'convenience']})
    #get_features('other_shops', {'shop': ['clothes', 'jewelry', 'shoes', 'tailor', 'beauty', 'cosmetics', 'hairdresser', 'doityourself', 'garden_center', 'hardware', 'mall', 'department_store']})
    #get_features('bus_stops', {'highway': 'bus_stop'})
    #get_features('restaurants', {'amenity': ['restaurant', 'bar', 'pub', 'cafe']})
    #get_features('parking', {'amenity': 'parking'})
    #get_features('bikePOI', {'amenity': ['bicycle_parking', 'bicycle_rental', 'bicycle_repair_station']})
    #get_features('greenspace', {'leisure': ['garden', 'nature_reserve', 'park', 'pitch']})
    #get_features('eduPOI', {'amenity': ['school', 'university', 'college']})
    #get_features('railway_stations', {'railway': ['station']})

def clip_features(name, poly, crs=None):
    if crs is None:
        crs = epsg
    poly_gdf = gpd.GeoDataFrame(geometry=[poly], crs=crs)
    return featuresdb[name].clip(poly_gdf.to_crs(4326).iloc[0].geometry)

# per sq km
def density_of_features(name, poly, crs=None, feats=None):
    if crs is None:
        crs = epsg
    poly_gdf = gpd.GeoDataFrame(geometry=[poly], crs=crs)
    poly_epsg = poly_gdf.iloc[0].geometry
    if feats is None:
        feats = clip_features(name, poly_epsg, crs=crs)
    return feats.shape[0] / poly_epsg.area * 1000000

def proportion_of_feature(name, poly, crs=None, feats=None):
    if crs is None:
        crs = epsg
    poly_gdf = gpd.GeoDataFrame(geometry=[poly], crs=crs)
    poly_epsg = poly_gdf.iloc[0].geometry
    if feats is None:
        feats = clip_features(name, poly_epsg, crs=crs)
    total_area = feats.to_crs(epsg).unary_union.area if feats.shape[0] > 0 else 0
    return total_area / poly_epsg.area

def shannon_entropy(name, poly, crs=None, feats=None):
    if crs is None:
        crs = epsg
    poly_gdf = gpd.GeoDataFrame(geometry=[poly], crs=crs)
    poly_epsg = poly_gdf.iloc[0].geometry
    if feats is None:
        feats = clip_features(name, poly_epsg, crs=crs)
    # Trim the table down to only the relevant_cols (if they exist), then find
    # the first non-NaN column and create a new table listing that first non-NaN
    # value for each row; turn this into value_counts.
    vc = feats.loc[:,feats.columns.isin(relevant_cols)].bfill(axis=1).iloc[:, 0].value_counts().to_numpy()
    # Shannon Entropy:
    return -np.sum(vc/np.sum(vc) * np.log(vc/np.sum(vc)))


basic_stat_keys_of_interest = ["num_nodes", "num_edges", "streets_per_node_avg", "streets_per_node_proportions1", "streets_per_node_proportions3", "streets_per_node_proportions4", "intersection_count", "street_length_total", "street_segment_count", "street_length_avg", "orientation_entropy", "median_speed"]

if args.features:
    feature_stats_path = basedir / Path(f'feature_stats_buffer{buffer_size}m.gpkg')
    if not feature_stats_path.exists():
        t0 = time.time()
        gdf_len = buffered_gdf.shape[0]
        keys = featuresdb.keys()
        if not args.write_only:
            results = {f'{k}_count': [] for k in keys}
            results['image_id'] = gdf.image_id.to_list()
            results['geometry'] = gdf.geometry.to_list()
            results.update({f'{k}_density_or_proportion': [] for k in keys})
            results.update({f'{k}_shannon_entropy': [] for k in keys})
            results['bike_network_length'] = []
            results['walk_network_length'] = []
            results.update({k: [] for k in basic_stat_keys_of_interest})
            print(f'Gathering results for: {",".join(results.keys())}')
        log(f'Feature kinds: {",".join(keys)}')
        log(f'Clipping {len(keys)} features for {gdf_len} polygons.')
        for i in tqdm(range(gdf_len)):
            for k in keys:
                k_path = basedir / Path(f'features_buffer{buffer_size}m') / Path(k)
                k_path.mkdir(parents=True, exist_ok=True)

                vc = featuresdb[k]['geometry'].type.value_counts()
                use_proportion = 'olygon' in vc[vc == vc.max()].keys()[0]

                features_path = k_path / Path(f'{i:06d}.geojson')
                features_path_bz2 = k_path / Path(f'{i:06d}.geojson.bz2')
                poly = buffered_gdf.iloc[i].geometry
                if features_path.exists():
                    if not args.write_only:
                        feats = gpd.read_file(features_path)
                elif features_path_bz2.exists():
                    if not args.write_only:
                        with bz2.open(features_path_bz2) as fp:
                            feats = gpd.read_file(fp)
                else:
                    feats = clip_features(k, poly)
                    if not feats.is_empty.all():
                        with bz2.open(features_path_bz2, 'w') as fp:
                            feats.to_file(fp, driver="GeoJSON")
                if not args.write_only:
                    results[f'{k}_count'].append(feats.shape[0])
                    results[f'{k}_density_or_proportion'].append(proportion_of_feature(k, poly) if use_proportion else density_of_features(k, poly))
                    results[f'{k}_shannon_entropy'].append(shannon_entropy(k, poly))

            def subgraphpath(ty): return subgraphsdir / Path(ty) / Path(f'{i:06d}.graphml')
            if subgraphpath('all').exists():
                allsubG = ox.io.load_graphml(subgraphpath('all'))
                bs = basic_stats(allsubG)
                bs['orientation_entropy'] = float(orientation_entropy(allsubG))
                bs['median_speed'] = float(calc_medspeed(allsubG))
                for statkey in basic_stat_keys_of_interest:
                    results[statkey].append(bs.get(statkey,0))
            if subgraphpath('bike').exists():
                bikesubG = ox.io.load_graphml(subgraphpath('bike'))
                bs = basic_stats(bikesubG)
                results['bike_network_length'].append(bs.get('street_length_total',0))
            if subgraphpath('walk').exists():
                walksubG = ox.io.load_graphml(subgraphpath('walk'))
                bs = basic_stats(walksubG)
                results['walk_network_length'].append(bs.get('street_length_total',0))

        t1 = time.time()
        log(f'clip_features (all kinds, all polygons): {t1 - t0:.2f}s elapsed')
        if not args.write_only: # because the needed data was not collected in write-only mode
            feat_gdf = gpd.GeoDataFrame(results, crs=gdf.crs)
            feat_gdf.to_file(feature_stats_path, driver='GPKG')


if args.stats_gdf_output:
    t0 = time.time()
    stats_gdf_output_path = basedir / Path(f'stats_buffer{buffer_size}m.gpkg')
    gdf_len = buffered_gdf.shape[0]
    keys_of_interest = [ "streets_per_node_avg", "streets_per_node_proportions1", "streets_per_node_proportions3", "streets_per_node_proportions4", "intersection_count", "street_length_total", "street_length_avg", "orientation_entropy", "median_speed" ]

    results = {k: [] for k in keys_of_interest}
    results['walk_length_total'] = []
    results['bike_length_total'] = []
    results['image_id'] = gdf.image_id.to_list()
    results['geometry'] = gdf.geometry.to_list()
    for i in tqdm(range(gdf_len)):
        subgraphstatspaths = {
            'all': subgraphsdir / Path('all') / Path(f'{i:06d}_stats.json'),
            'walk': subgraphsdir / Path('walk') / Path(f'{i:06d}_stats.json'),
            'bike': subgraphsdir / Path('bike') / Path(f'{i:06d}_stats.json')
        }
        subgraphstatsjs = {}
        for nt, p in subgraphstatspaths.items():
            if not p.exists():
                log(f'Unable to find subgraph stats for row {i} network_type={nt}. Did you run --subgraph-stats with --subgraphs {nt} for --buffer-size {buffer_size}?', level=0)
                sys.exit(1)
            with open(p) as fp:
                subgraphstatsjs[nt] = json.load(fp)
        for k in keys_of_interest:
            results[k].append(subgraphstatsjs['all'].get(k, 0))
        results['walk_length_total'].append(subgraphstatsjs['walk'].get('street_length_total',0))
        results['bike_length_total'].append(subgraphstatsjs['bike'].get('street_length_total',0))

    stats_gdf = gpd.GeoDataFrame(results, crs=gdf.crs)
    t1 = time.time()
    stats_gdf.to_file(stats_gdf_output_path, driver='GPKG')
    log(f'stats_gdf_output (written to {stats_gdf_output_path}): {t1 - t0:.2f}s elapsed')


    # Download the input GeoJSON data (just in case it wasn't already downloaded)
    #response = requests.get(url)
    #js = response.json()



##################################################
#for i, subG in enumerate(subGs):
    #ox.io.save_graphml(subG, subgraphsdir / Path(f'{i:06d}.graphml'))

#show_on_map(ox.graph_to_gdfs(subGs[1000], nodes=False, edges=True))

